{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Data science with Kiln\n",
    "\n",
    "This notebook demonstrates how the data generated by Kiln can be used to gain insights into how quickly Open Source dependency vulnerabilities are remediated by projects, allowing you to focus your effort on projects that pose more risk to your organisation.\n",
    "\n",
    "The data for this demo was gathered from the [OWASP RailsGoat](https://github.com/OWASP/railsgoat), [Mastodon](https://github.com/tootsuite/mastodon) and [GitLab](https://gitlab.com/gitlab-org/gitlab) codebases. Kiln was run on each commit to the default branch of each of these projects, from the very first until the most recent as of 2020/01/18. RailGoat serves as a point of comparison and is a deliberately vulnerable application, whereas Mastodon and GitLab are fairly large codebases using in production. All three are Ruby projects, which when this demo was originally built, was the only language that Kiln had tools available for.\n",
    "\n",
    "Data is read from the Kiln Kafka cluster using Apache Spark, then most of the heavy lifting is done by the Python Data Analysis library, Pandas.\n",
    "The data coming from Kafka is GZip compressed and encoded using Apache Avro, Spark handles decompressing the data, and the FastAvro library is used to get the data back into a useable form.\n",
    "\n",
    "The Avro decoding is a heavily CPU intensive task, and because this was written to analyse ~1,000,000 events which are all read into memory, it also also fairly memory intensive. To speed up the Avro decoding, the Python Multiprocessing library is used to parallelise this task over a number of cores. This was tested on an AWS T3a.2xlarge EC2 instance and found to perform acceptably.\n",
    "\n",
    "For each CVE found in each project, we record the commit that introduced the vulnerable library and the last commit it was seen in. Then we can use the PyGit2 library to find the next commit to the default branch that did not contain the vulnerable package, and record the time this commit was made. Next we load data from the [NIST NVD](https://nvd.nist.gov/vuln/data-feeds) to find when the CVE was published. We can then use the CVE publication date and remediation date to determine how long that vulnerability was present for. We filter out results with a negative remediation time, because this indicates that the package was upgraded in the default branch before the CVE was discovered. From there, we can calculate the some basic statistics on the remediation times for each project and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import ChainMap\n",
    "from functools import partial\n",
    "from io import BytesIO\n",
    "from multiprocessing import Pool\n",
    "from pygit2 import GIT_SORT_REVERSE, GIT_SORT_TOPOLOGICAL, Repository,Oid\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "import datetime\n",
    "import dateutil\n",
    "import fastavro\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas\n",
    "import requests\n",
    "import string\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEMORY = \"28g\"\n",
    "os.environ['PYTHON_PATH'] = '$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.8.1-src.zip'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0-preview2 pyspark-shell'\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"kiln-analysis\") \\\n",
    "    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka-headless:9092\") \\\n",
    "  .option(\"subscribe\", \"DependencyEvents\") \\\n",
    "  .option(\"compression\", \"deflate\") \\\n",
    "  .load()\n",
    "pdf = df.select(df.value).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_partitions = 30\n",
    "num_cores = 7\n",
    "\n",
    "def parallelize_dataframe(df, func):\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    with Pool(num_cores) as pool:\n",
    "        df = pandas.concat(pool.map(func, df_split))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        return df\n",
    "\n",
    "def parallel_parse(df):\n",
    "    return df.apply(lambda val: parse_avro(val), axis='columns')\n",
    "\n",
    "def parse_avro(val):\n",
    "    buf = BytesIO(val['value'])\n",
    "    reader = fastavro.reader(buf)\n",
    "    return [rec for rec in reader]\n",
    "\n",
    "parsed_pdf = parallelize_dataframe(pdf, parallel_parse).explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pandas.DataFrame.from_records(parsed_pdf.apply(lambda val: dict(val)))\n",
    "data['timestamp'] = pandas.to_datetime(data['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = data.groupby(['application_name'])\n",
    "print(grouped_data.size())\n",
    "grouped_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "railsgoat_data = grouped_data.get_group('railsgoat')\n",
    "mastodon_data = grouped_data.get_group('mastodon')\n",
    "gitlab_data = grouped_data.get_group('gitlab')\n",
    "\n",
    "def get_cve_data(raw_data, app_name):\n",
    "    cve_ids = raw_data[raw_data['advisory_id'].str.startswith('CVE')]['advisory_id'].unique()\n",
    "    cve_data = dict()\n",
    "    repo = Repository(f\"/home/jovyan/{app_name}.git\")\n",
    "    walker = repo.walk(repo.head.target, GIT_SORT_TOPOLOGICAL | GIT_SORT_REVERSE)\n",
    "    walker.simplify_first_parent()\n",
    "    repo_commits = [str(commit.id) for commit in walker]\n",
    "    for cve in cve_ids:\n",
    "        commits = raw_data[raw_data['advisory_id'] == cve].sort_values('timestamp')\n",
    "        first_commit = commits.head(1)['git_commit_hash'].values[0]\n",
    "        last_commit = commits.tail(1)['git_commit_hash'].values[0]\n",
    "        try:\n",
    "            fixing_commit_parent = repo_commits.index(last_commit)\n",
    "            fixing_commit = repo.get(Oid(hex=repo_commits[fixing_commit_parent + 1]))\n",
    "            fixing_commit_id = fixing_commit.id\n",
    "            fixing_time_offset = datetime.timedelta(minutes=fixing_commit.commit_time_offset)\n",
    "            original_fixing_date = datetime.datetime.fromtimestamp(fixing_commit.commit_time, datetime.timezone(fixing_time_offset))\n",
    "            fix_date = datetime.datetime.astimezone(original_fixing_date, datetime.timezone.utc)\n",
    "        except:\n",
    "            fixing_commit_id = None\n",
    "            fix_date = None        \n",
    "        cve_data[cve] = {\"first_commit\": first_commit, \"fixing_commit\": fixing_commit_id, \"fix_date\": fix_date}\n",
    "    return cve_data\n",
    "\n",
    "railsgoat_cve_data = get_cve_data(railsgoat_data, \"railsgoat\")\n",
    "mastodon_cve_data = get_cve_data(mastodon_data, \"mastodon\")\n",
    "gitlab_cve_data = get_cve_data(gitlab_data, \"gitlab\")\n",
    "\n",
    "print(f\"Railsgoat has had {len(railsgoat_cve_data.keys())} CVEs land in the default branch\")\n",
    "print(f\"Mastodon has had {len(mastodon_cve_data.keys())} CVEs land in the default branch\")\n",
    "print(f\"Gitlab has had {len(gitlab_cve_data.keys())} CVEs land in the default branch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_year = list()\n",
    "for year in itertools.chain(list(range(2002, 2020)), [\"modified\"]):\n",
    "    filename = f\"file:///home/jovyan/vulndata/nvdcve-1.1-{year}.json.gz\"\n",
    "    raw_json = pandas.read_json(filename)\n",
    "    parsed_vulns = dict()\n",
    "    for item in iter(raw_json['CVE_Items']):\n",
    "        parsed_vulns[item['cve']['CVE_data_meta']['ID']] = item['publishedDate']\n",
    "    data_by_year.append(parsed_vulns)\n",
    "nvd_cve_data = ChainMap(*data_by_year)\n",
    "\n",
    "for key, val in railsgoat_cve_data.items():\n",
    "    try:\n",
    "        val['cve_date'] = datetime.datetime.astimezone(dateutil.parser.parse(nvd_cve_data[key]), datetime.timezone.utc)\n",
    "    except:\n",
    "        val['cve_date'] = None\n",
    "        continue\n",
    "for key, val in mastodon_cve_data.items():\n",
    "    try:\n",
    "        val['cve_date'] = datetime.datetime.astimezone(dateutil.parser.parse(nvd_cve_data[key]), datetime.timezone.utc)\n",
    "    except:\n",
    "        val['cve_date'] = None\n",
    "        continue\n",
    "for key, val in gitlab_cve_data.items():\n",
    "    try:\n",
    "        val['cve_date'] = datetime.datetime.astimezone(dateutil.parser.parse(nvd_cve_data[key]), datetime.timezone.utc)\n",
    "    except:\n",
    "        val['cve_date'] = None\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "railsgoat_df = pandas.DataFrame(railsgoat_cve_data).transpose().dropna()\n",
    "mastodon_df = pandas.DataFrame(mastodon_cve_data).transpose().dropna()\n",
    "gitlab_df = pandas.DataFrame(gitlab_cve_data).transpose().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "railsgoat_df['remediation_time'] = railsgoat_df['fix_date'] - railsgoat_df['cve_date']\n",
    "mastodon_df['remediation_time'] = mastodon_df['fix_date'] - mastodon_df['cve_date']\n",
    "gitlab_df['remediation_time'] = gitlab_df['fix_date'] - gitlab_df['cve_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "railsgoat_positive_remediation_time = railsgoat_df.loc[railsgoat_df['remediation_time'].dt.total_seconds() > 0]\n",
    "mastodon_positive_remediation_time = mastodon_df.loc[mastodon_df['remediation_time'].dt.total_seconds() > 0]\n",
    "gitlab_positive_remediation_time = gitlab_df.loc[gitlab_df['remediation_time'].dt.total_seconds() > 0]\n",
    "\n",
    "print(f\"RailsGoat has had {len(railsgoat_positive_remediation_time.index)} vulnerable packages in the default branch\")\n",
    "print(f\"Mastodon has had {len(mastodon_positive_remediation_time.index)} vulnerable packages in the default branch\")\n",
    "print(f\"Gitlab has had {len(gitlab_positive_remediation_time.index)} vulnerable packages in the default branch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "railsgoat_positive_remediation_time['remediation_time'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mastodon_positive_remediation_time['remediation_time'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gitlab_positive_remediation_time['remediation_time'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mastodon_positive_remediation_time.sort_values('remediation_time').tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gitlab_positive_remediation_time.sort_values('remediation_time').tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
